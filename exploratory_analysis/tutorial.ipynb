{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"teaser\" style=' background-position:  right center; background-size: 00px; background-repeat: no-repeat; \n",
    "    padding-top: 20px;\n",
    "    padding-right: 10px;\n",
    "    padding-bottom: 170px;\n",
    "    padding-left: 10px;\n",
    "    border-bottom: 14px double #333;\n",
    "    border-top: 14px double #333;' > \n",
    "\n",
    "   \n",
    "   <div style=\"text-align:center\">\n",
    "    <b><font size=\"6.4\">Exploratory analysis of raw data using unsupervised learning</font></b>    \n",
    "  </div>\n",
    "    \n",
    "<p>\n",
    " created by:\n",
    " Luigi Sbailo<sup>1</sup> \n",
    " and Luca Ghiringhelli<sup>1</sup> <br><br>\n",
    "   \n",
    "<sup>1</sup> Fritz Haber Institute of the Max Planck Society, Faradayweg 4-6, D-14195 Berlin, Germany <br>\n",
    "\n",
    "  \n",
    "<div> \n",
    "<img  style=\"float: left;\" src=\"assets/exploratory_analysis/Logo_MPG.png\" width=\"200\"> \n",
    "<img  style=\"float: right;\" src=\"assets/exploratory_analysis/Logo_NOMAD.png\" width=\"250\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we use unsupervised learning for a preliminary exploration of materials science data. More specifically, we analyze 82 octet binary materials known to crystallize in zinc blende (ZB) and rocksalst (RS) structures. Our aim is to identify the right strategy to facilitate the visualization and characterization of unlabeled data. As a first step in our data analysis, we would like to detect whether data points can be classified into different  clusters, where each cluster is aimed to group together objects that share similar features. With an explorative analysis we would like to visualize the structure and spatial displacement of the clusters, but when the feature space is higlhly multidimensional such visualization is directly not possible. Hence, we project the feature space into a two-dimensional manifold that can be  visualized. To avoid losing relevant information, the embedding into a lower dimensional manifold must be performed while preserving the most informative features in the original space. Below we introduce into different clustering and embedding methods, which can be combined to obtain different visualizations of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis is performed to group together data points that are more similar to each other in comparison with points belonging to other clusters. Clustering can be achieved by means of many different algorithms, each with proper characteristics and input parameters. The choice of the specific clustering algorithms to be used depends on the individual data set analyzed, and, once an optimal algorithm has been chosen, it is often necessary to iteratively modify the input parameters until results achieve the desired properties. We focus on three distinct algorithms as described below.\n",
    "- ___k_-means__ partitions the data set into _k_ clusters, where each data point belongs to the cluster with the nearest mean. This partition ultimately minimizes the within-cluster variance to find the most compact partitioning of the data set. _K_-means uses an iterative refinement technique that is fast and scalable, but if falls in local minima. Thus, the algorithm is iterated multiple times with different initial conditions and the best outcome is finally chosen. Drawbacks of this algorithm are that the number of clusters _k_ is an input parameter which must be known in advance and clusters are convex shaped.\n",
    "- Density-based spatial clustering of applications with noise (__DBSCAN__) is an algorithm that, without knowing the exact number of clusters, groups points that are close to each other leaving outliers marked as noise and not defined in any cluster. In this algorithm a neighborood distance _$\\epsilon$_  and a number of points _min-samples_ are used to determine if a point belongs to a cluster: if the point has a number _min-samples_ of other points  within the distance _$\\epsilon$_ is marked as core point and belongs to a cluster; otherwise, the point is marked as noise. This algorithm is fast and clusters can assume any shape, but the outcome depends on the initial order of the data points.\n",
    "- __Hierarchical clustering__ builds a hierarchy of clusters with a bottom-up (__agglomerative__) or top-down (__divisive__) approach. In a bottom-up approach, that we deploy below, starting with all data points placed in its own cluster, different pairs of clusters are iteratively merged together where the decision of the clusters to be merged is determined in a greedy manner. This is iterated until all points are grouped within one cluster, and the resulting hierarchy of clusters is presentend in a dendogram. Given a distance thereshold it is possible to avoid merging of clusters when outside this distance, this stops the algorithm when no more mergings are possible. The algorithm then returns a certain number of clusters as a function of the threshold distance . An advantage of this algorithm is that the construction of dendroids allows for a visual inspection of the clustering, but hierarchical clustering is considerably slower than the other algorithms discussed above and not well suited for big data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of a dataset is not possible when it is defined in a highly multidimensional space. To facilitate visualization of inner structures in the dataset, we reduce the dimensionality of the system with methodologies specifically developed to avoid losing critical information, which are introduced below.\n",
    "- Principal component analysis (__PCA__) is a linear projection method that seeks for an orthogonal transformation of the dataset so as to render the variables of the dataset uncorrelated. The dimensionality reduction is then performed onto the features with highest variance to preserve as much information as possible. This is a deterministic but linear method, that fails to catch non linear correlations.\n",
    "- Multi-dimensional scaling (__MDS__) constructs a pairwise distance matrix in the original space, and seeks a low-dimensional representation that preserves the original distances as much as possible. This method tends to preserve local structures better than global structures and scales badly with the number of the data points. \n",
    "- T-distributed Stochastic Neighbor Embedding (__t-SNE__) is a non-linear dimensionality reduction method that converts similarities between data points to joint probabilities and minimizes the Kullback-Leibler divergence between the joint probabilities of the embedding and the original space. The cost function is not convex and results depend on the inizialization. Non linear effects in this method might occasionally produce misleading results, a fine parameter tuning and several iterations of the method are then recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pydpc import Cluster as DPCClustering\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.df_flag = False\n",
    "        try:\n",
    "            df \n",
    "        except NameError:\n",
    "            print(\"Please define a dataframe 'df'\")\n",
    "            self.df_flag = True \n",
    "        try:\n",
    "            features\n",
    "        except NameError:\n",
    "            print(\"Please define the features to extract from the dataframe\")\n",
    "            self.df_flag = True\n",
    "    \n",
    "    def kmeans (self, n_clusters, max_iter):\n",
    "        if self.df_flag: \n",
    "            return \n",
    "        cluster_labels = KMeans (n_clusters=n_clusters, max_iter=max_iter).fit_predict(df[features])\n",
    "        df['clustering'] = 'k-means'\n",
    "        df['labels']=cluster_labels\n",
    "\n",
    "    def hierarchical (self, distance_threshold):\n",
    "        if self.df_flag: \n",
    "            return \n",
    "        cluster_labels = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold).fit_predict(df[features])\n",
    "        df['clustering'] = 'Hierarchical'\n",
    "        df['labels']=cluster_labels\n",
    "\n",
    "    def dbscan (self, eps, min_samples):\n",
    "        if self.df_flag: \n",
    "            return \n",
    "        cluster_labels = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(df[features])\n",
    "        df['clustering'] = 'DBSCAN'\n",
    "        df['labels']=cluster_labels\n",
    "    \n",
    "    def dpc (self, density = 0, delta = 0  ):\n",
    "        if self.df_flag:\n",
    "            return\n",
    "        if density > 0 and delta > 0 :\n",
    "            clu=DPCClustering(np.ascontiguousarray(df[features].to_numpy()), autoplot=False)\n",
    "            clu.autoplot = True\n",
    "            clu.assign(density,delta)\n",
    "            cluster_labels = clu.membership\n",
    "            df['clustering'] = 'DPC'\n",
    "            df['labels']=cluster_labels\n",
    "            df[\"labels\"]=df[\"labels\"].astype(str)\n",
    "        else: \n",
    "            clu=DPCClustering(np.ascontiguousarray(df[features].to_numpy()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold=15\n",
    "Clustering().hierarchical(distance_threshold=distance_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_PCA = widgets.Button(description='PCA')\n",
    "btn_MDS = widgets.Button(description='MDS')\n",
    "btn_tSNE = widgets.Button(description='t-SNE')\n",
    "btn_kmeans = widgets.Button(description='k-means')\n",
    "btn_hierarchical = widgets.Button(description='hierarchical')\n",
    "btn_dbscan = widgets.Button(description='DBSCAN')\n",
    "btn_plot = widgets.Button (description='plot')\n",
    "\n",
    "\n",
    "def btn_eventhandler_embedding (obj):\n",
    "    method = str (obj.description)\n",
    "    print(method)\n",
    "    if (method == 'PCA'):\n",
    "        transformed_data = PCA(n_components=2).fit_transform(df[features])\n",
    "        df['x_emb']=transformed_data[:,0]\n",
    "        df['y_emb']=transformed_data[:,1]\n",
    "        df['embedding'] = 'PCA'\n",
    "    elif (method == 'MDS'):\n",
    "        transformed_data = MDS (n_components=2).fit_transform(df[features])\n",
    "        df['x_emb']=transformed_data[:,0]\n",
    "        df['y_emb']=transformed_data[:,1]\n",
    "        df['embedding'] = 'MDS'\n",
    "    elif (method == 't-SNE'):\n",
    "        transformed_data = TSNE (n_components=2).fit_transform(df[features])\n",
    "        df['x_emb']=transformed_data[:,0]\n",
    "        df['y_emb']=transformed_data[:,1]\n",
    "        df['embedding'] = 't-SNE'\n",
    "    \n",
    "    \n",
    "def btn_eventhandler_plot (obj):\n",
    " \n",
    "    try:\n",
    "        df \n",
    "    except NameError:\n",
    "        print(\"Please define a dataframe 'df'\")\n",
    "        return 0\n",
    "    try:\n",
    "        df['clustering'][0]\n",
    "    except KeyError:\n",
    "        print(\"Please assign labels with a clustering algorithm\")\n",
    "        return 0\n",
    "    try:\n",
    "        df['embedding'][0]\n",
    "    except KeyError:\n",
    "        print(\"Please select an embedding method\")        \n",
    "        return 0\n",
    "    else:\n",
    "        print (\"Clustering algorithm used: \",df['clustering'][0], \"\\t Embedding method used: \", df['embedding'][0])\n",
    "        df[\"labels\"]=df[\"labels\"].astype(str)\n",
    "        display(px.scatter(df,x='x_emb',y='y_emb',color='labels',hover_data=df[hover_features], hover_name=df.index ))\n",
    "\n",
    "    \n",
    "btn_PCA.on_click(btn_eventhandler_embedding)\n",
    "btn_MDS.on_click(btn_eventhandler_embedding)\n",
    "btn_tSNE.on_click(btn_eventhandler_embedding)\n",
    "\n",
    "btn_plot.on_click(btn_eventhandler_plot)\n",
    "\n",
    "left_box = widgets.VBox ([btn_PCA,btn_MDS,btn_tSNE])\n",
    "box = widgets.HBox ([left_box,btn_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Let us load the data from the file data/data.pkl into a data frame. The data was downloaded from the NOMAD archive and the NOMAD atomic data collection. It consists of RS-ZB energy differences (in eV/atom) of the 82 octet binary compounds, structure objects containing the atomic positions of the materials and properties of the atomic constituents. The following atomic features are considered:\n",
    "\n",
    "- Z:  atomic number\n",
    "- period: period in the periodic table\n",
    "- IP: ionization potential\n",
    "- EA: electron affinity\n",
    "- E_HOMO: energy of the highest occupied atomic orbital\n",
    "- E_LUMO: energy of the lowest unoccupied atomic orbital\n",
    "- r_(s, p, d): radius where the radial distribution of s, p or d orbital has its maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "RS_structures = read(\"assets/exploratory_analysis/octet_binaries/RS_structures.xyz\", index=':')\n",
    "ZB_structures = read(\"assets/exploratory_analysis/octet_binaries/ZB_structures.xyz\", index=':')\n",
    "\n",
    "def generate_table(RS_structures, ZB_structures):\n",
    "\n",
    "    for RS, ZB in zip(RS_structures, ZB_structures):\n",
    "        energy_diff = RS.info['energy'] - ZB.info['energy']\n",
    "        min_struc_type = 'RS' if energy_diff < 0 else 'ZB'\n",
    "        struc_obj_min = RS if energy_diff < 0 else ZB\n",
    "\n",
    "        yield [RS.info['energy'], ZB.info['energy'],\n",
    "               energy_diff, min_struc_type,\n",
    "               RS.info['Z'], ZB.info['Z'],\n",
    "               RS.info['period'], ZB.info['period'],\n",
    "               RS.info['IP'], ZB.info['IP'],\n",
    "               RS.info['EA'], ZB.info['EA'],\n",
    "               RS.info['E_HOMO'], ZB.info['E_HOMO'],\n",
    "               RS.info['E_LUMO'], ZB.info['E_LUMO'],\n",
    "               RS.info['r_s'], ZB.info['r_s'],\n",
    "               RS.info['r_p'], ZB.info['r_p'],\n",
    "               RS.info['r_d'], ZB.info['r_d']]\n",
    "        \n",
    "    \n",
    "df = pd.DataFrame(\n",
    "    generate_table(RS_structures, ZB_structures),\n",
    "    columns=['energy_RS', 'energy_ZB', \n",
    "             'energy_diff', 'min_struc_type', \n",
    "             'Z(A)', 'Z(B)', \n",
    "             'period(A)', 'period(B)', \n",
    "             'IP(A)', 'IP(B)', \n",
    "             'EA(A)', 'EA(B)', \n",
    "             'E_HOMO(A)', 'E_HOMO(B)', \n",
    "             'E_LUMO(A)', 'E_LUMO(B)', \n",
    "             'r_s(A)', 'r_s(B)', \n",
    "             'r_p(A)', 'r_p(B)', \n",
    "             'r_d(A)', 'r_d(B)',],\n",
    "    index=list(RS.get_chemical_formula() for RS in RS_structures)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select which features will be used for the clustering and embedding methods. The complexity of the problem clearly is reduced with lowering the number of features that are considered, and an accurate selection of the features to be processed can imporove the quality of the results. To find the most meaningful results it is sometimes necessary to iterate training while considering different features at each iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append('IP(A)')\n",
    "features.append('IP(B)')\n",
    "features.append('EA(A)')\n",
    "features.append('EA(B)')\n",
    "features.append('E_HOMO(A)')\n",
    "features.append('E_HOMO(B)')\n",
    "features.append('E_LUMO(A)')\n",
    "features.append('E_LUMO(B)')\n",
    "features.append('r_s(A)')\n",
    "features.append('r_s(B)')\n",
    "features.append('r_p(A)')\n",
    "features.append('r_p(B)')\n",
    "features.append('r_d(A)')\n",
    "features.append('r_d(B)')\n",
    "\n",
    "hover_features = ['min_struc_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms can improve their performance if data is standardized. In fact, training can be biased towards dimensions presenting higher absolute values, or outliers can undermine the learning capabilites  of the algorithm. Hence, we standardize our dataset by subtracting the mean value and dividing it by the standard deviation for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[features]=preprocessing.scale(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = df[features].hist( bins=10, figsize = (20,15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means requires the knowledge of the number of clusters and clustering depends on the initial conditions, hence the algorithm is iterated,  up to _max\\_iter_ times, with different initial conditions until convergence. As initial guess we seek for 2 clusters and run the algorithm up to 200 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "max_iter = 200\n",
    "Clustering().kmeans(n_clusters, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you identify and visualize two distinct clusters within your data? If not the number of clusters must be changed among the input parameters. You can also run the k-means clustering again and select only 1 as _max\\_iter_ , which means that the first output is taken as optimal result. Try this again and compare the results, does the output change at each iteration? What happens instead if the number is much larger?\n",
    "\n",
    "Note that also MDS and t-SNE are stochastich algorithms, so it might be worth iterate also the embedding to find a more satisfying result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant parameter of DBSCAN is the maximum distance $\\epsilon$ that determines the extent of the cluster and whether a point is considered as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 3\n",
    "min_samples= 5\n",
    "Clustering().dbscan(eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the number of clusters that you have just found the same as the one that you used in k-means? Can you spot the noise in the visualization? What happens lowering the maximal distance $\\epsilon$?\n",
    "\n",
    "MDS seeks for an embedding that tries to preserve pairwise distances. Can you notice that noise is distant from other clusters? Does the same happen with t-SNE? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical agglomerative clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a hierarchical agglomerative clustering different clusters are iteratively merged if their distance is lower than a _distance\\_threshold_. The number of clusters obtained is a function of this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold=5\n",
    "Clustering().hierarchical(distance_threshold=distance_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several different possible metrics can be used for the linkage criterium. As a default option, we have used the Ward distance, which minimizes the sum of squared differences within all clusters. This has some similarities with the objective function of _k-means_, but tackled differently. By tuning the parameters, can you find the same results using the two different methodologies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try again including different features. Is the classification obtained identical?\n",
    "You can hover over the different plots and explore the classification of the materials.\n",
    "Can you identify  meaningful clusters that group together materials that share similar properties? What clustering and embedding methods provide the most meaningful visualization of the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=15, n_clusters=None)\n",
    "\n",
    "model = model.fit(df[features].to_numpy())\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast search and find of density peaks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering().dpc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering().dpc(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append('Z(A)')\n",
    "features.append('Z(B)')\n",
    "features.append('IP(A)')\n",
    "features.append('IP(B)')\n",
    "# features.append('EA(A)')\n",
    "# features.append('EA(B)')\n",
    "features.append('E_HOMO(A)')\n",
    "features.append('E_HOMO(B)')\n",
    "features.append('E_LUMO(A)')\n",
    "features.append('E_LUMO(B)')\n",
    "features.append('r_s(A)')\n",
    "features.append('r_s(B)')\n",
    "features.append('r_p(A)')\n",
    "features.append('r_p(B)')\n",
    "features.append('r_d(A)')\n",
    "features.append('r_d(B)')\n",
    "\n",
    "hover_features = ['min_struc_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split ( df, test_size=0.2)\n",
    "data_train = df_train.filter(items=features).to_numpy()\n",
    "data_train = preprocessing.scale(data_train)\n",
    "data_test = df_test.filter(items=features).to_numpy()\n",
    "data_test = preprocessing.scale(data_test)\n",
    "cluster_labels_train = KMeans (n_clusters=2).fit_predict(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf=svm.SVC(probability=True)\n",
    "clf.fit(data_train,cluster_labels_train)\n",
    "labels_test=clf.predict(data_test)\n",
    "df_test['labels']=labels_test\n",
    "df_test[\"labels\"]=df_test[\"labels\"].astype(str)\n",
    "\n",
    "transformed_data = PCA(n_components=2).fit_transform(data_test)\n",
    "df_test['x_emb']=transformed_data[:,0]\n",
    "df_test['y_emb']=transformed_data[:,1]\n",
    "px.scatter(df_test,x='x_emb',y='y_emb',color='labels',hover_data=df[hover_features], hover_name=df_test.index )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perovskite\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"assets/exploratory_analysis/perovskites/cubic_perosvkistes_data.asc\", sep=' ')\n",
    "df = df.dropna(axis=1)\n",
    "df['type'] = df.apply(lambda x : 'metallic' if  x['band_gap'] < 0.2  else 'non metallic' , axis=1)\n",
    "\n",
    "features = df.drop(['band_gap','material','type'],axis=1).columns.tolist()\n",
    "hover_features = ['type','material']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Nomad\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad import client, config\n",
    "config.client.url = 'http://labdev-nomad.esc.rzg.mpg.de/dev/nomad/v0-8-0/api'\n",
    "results = client.query_archive(query={},\n",
    "                              per_page=10, max=500)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.DataFrame(columns=['Energy'])\n",
    "for i, e in enumerate(results):\n",
    "    element = e.section_run[0].section_system[1].chemical_composition\n",
    "    row = pd.Series({'Energy': e.section_run[0].section_single_configuration_calculation[0].energy_total.to_tuple()[0]}, name=str(element))\n",
    "    df=df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Energy']=preprocessing.scale(df['Energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df.hist( bins=50, figsize = (10,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
